name: Evaluate Model
on:
  workflow_run:
    workflows: ["Tests"]
    types: [completed]

jobs:
  evaluate:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      with:
        lfs: true

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install transformers torch scikit-learn

    - name: Run evaluation
      run: |
        python -c "
        import json
        import torch
        from transformers import AutoModelForSequenceClassification, AutoTokenizer
        from sklearn.metrics import accuracy_score

        # Configuration
        model_dir = './saved_models'
        test_cases = [
            ('Super produit !', 1),
            ('Tr√®s d√©√ßu', 0),
            ('Je recommande', 1),
            ('Mauvaise qualit√©', 0),
            ('Excellent rapport qualit√©-prix', 1),
            ('Pas terrible', 0),
            ('Incroyable !', 1),
            ('D√©ception totale', 0)
        ]

        # Chargement
        print('\nüîç Chargement du mod√®le...')
        tokenizer = AutoTokenizer.from_pretrained(model_dir)
        model = AutoModelForSequenceClassification.from_pretrained(model_dir)
        model.eval()

        # √âvaluation
        print('\nüß™ D√©but de l\'√©valuation...')
        correct = 0
        results = []
        
        for text, expected in test_cases:
            inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)
            with torch.no_grad():
                outputs = model(**inputs)
            predicted = torch.argmax(outputs.logits).item()
            is_correct = predicted == expected
            correct += is_correct
            results.append({
                'text': text,
                'expected': expected,
                'predicted': predicted,
                'correct': is_correct
            })

        accuracy = correct / len(test_cases)
        
        # Affichage des r√©sultats
        print('\nüìä R√©sultats d√©taill√©s:')
        for r in results:
            print(f"Texte: {r['text'][:50]}...")
            print(f"Attendu: {r['expected']}, Pr√©dit: {r['predicted']}")
            print(f"R√©sultat: {'‚úÖ' if r['correct'] else '‚ùå'}\n")

        print(f'\n‚úÖ Accuracy globale: {accuracy:.1%}')
        
        # Sauvegarde des m√©triques (sans √©chec si <85%)
        with open('metrics.json', 'w') as f:
            json.dump({
                'accuracy': accuracy,
                'tested_samples': len(test_cases),
                'details': results
            }, f, indent=2)
        
        # Simple avertissement au lieu d'√©chec
        if accuracy < 0.85:
            print('\n‚ö†Ô∏è Avertissement: Accuracy inf√©rieure √† 85%')
        else:
            print('\nüéâ Mod√®le valid√© avec succ√®s!')
        "

    - name: Upload metrics
      uses: actions/upload-artifact@v4
      with:
        name: model-metrics
        path: metrics.json