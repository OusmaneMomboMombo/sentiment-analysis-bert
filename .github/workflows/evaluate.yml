name: Evaluate Model
on:
  workflow_run:
    workflows: ["Tests"]
    types: [completed]

jobs:
  evaluate:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: pip install -r requirements.txt torch scikit-learn

    - name: Quick evaluation (no external file needed)
      run: |
        python -c "
        import json
        from transformers import pipeline
        
        # Chargez le modèle directement
        classifier = pipeline('text-classification', 
                            model='./saved_models/bert_sentiment.pth',
                            tokenizer='bert-base-uncased')
        
        # Test simplifié avec 2 phrases
        test_samples = [
            {'text': 'J\'adore ce film', 'label': 1},
            {'text': 'Je déteste ce restaurant', 'label': 0}
        ]
        
        correct = 0
        for sample in test_samples:
            pred = classifier(sample['text'])[0]
            if (pred['label'] == 'POSITIVE' and sample['label'] == 1) or \
               (pred['label'] == 'NEGATIVE' and sample['label'] == 0):
                correct += 1
        
        accuracy = correct / len(test_samples)
        print(f'Accuracy: {accuracy:.2f}')
        
        # Enregistrez les métriques
        with open('metrics.json', 'w') as f:
            json.dump({'accuracy': accuracy}, f)
            
        # Échoue si < 85%
        assert accuracy >= 0.85, 'Performance insuffisante'
        "

    - uses: actions/upload-artifact@v4
      with:
        name: model-metrics
        path: metrics.json