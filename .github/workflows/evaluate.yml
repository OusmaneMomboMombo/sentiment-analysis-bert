name: Evaluate Model
on:
  workflow_run:
    workflows: ["Tests"]
    types: [completed]

jobs:
  evaluate:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install torch transformers pandas scikit-learn
        echo "Dépendances installées"

    - name: Create evaluation script
      run: |
        cat << 'EOF' > evaluate.py
        import json
        import pandas as pd
        from transformers import pipeline
        from sklearn.metrics import accuracy_score

        # Charger modèle et données
        model = pipeline("text-classification", model="saved_models/bert_sentiment.pth")
        test_data = pd.DataFrame({
            "text": ["J'adore ce film", "Je déteste ce restaurant"],
            "label": [1, 0]  # 1=positif, 0=négatif
        })

        # Évaluation
        predictions = model(test_data["text"].tolist())
        pred_labels = [1 if p["label"].upper() == "POSITIVE" else 0 for p in predictions]
        accuracy = accuracy_score(test_data["label"], pred_labels)

        # Seuil de validation (à adapter)
        threshold = 0.85
        metrics = {
            "accuracy": accuracy,
            "threshold": threshold,
            "status": "SUCCESS" if accuracy >= threshold else "FAILED"
        }

        # Sauvegarder les résultats
        with open("metrics.json", "w") as f:
            json.dump(metrics, f)

        # Échouer si en dessous du seuil
        if accuracy < threshold:
            raise ValueError(f"Accuracy {accuracy:.2f} < threshold {threshold}")
        EOF

    - name: Run evaluation
      run: |
        python evaluate.py
        cat metrics.json

    - uses: actions/upload-artifact@v4
      with:
        name: model-evaluation
        path: metrics.json