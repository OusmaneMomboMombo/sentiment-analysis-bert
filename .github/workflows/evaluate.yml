name: Evaluate Model
on:
  workflow_run:
    workflows: ["Tests"]
    types: [completed]

jobs:
  evaluate:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: pip install -r requirements.txt torch transformers

    - name: Run real evaluation
      run: |
        python -c "
        import json
        from transformers import pipeline

        # Phrases de test avec labels attendus
        test_cases = [
            ('J\'adore ce film, il est génial', 'POSITIVE'),
            ('Le service client est horrible', 'NEGATIVE'),
            ('Très bon rapport qualité-prix', 'POSITIVE'),
            ('Je ne recommande pas ce produit', 'NEGATIVE'),
            ('Livraison ultra rapide, merci !', 'POSITIVE')
        ]

        # Charge le modèle
        classifier = pipeline('text-classification', 
                            model='./saved_models/',
                            tokenizer='bert-base-uncased')

        # Calcule la précision
        correct = 0
        for text, expected in test_cases:
            pred = classifier(text)[0]['label']
            if pred == expected:
                correct += 1
        accuracy = correct / len(test_cases)

        # Sauvegarde les résultats
        with open('metrics.json', 'w') as f:
            json.dump({'accuracy': accuracy, 'tested_samples': len(test_cases)}, f)

        # Échoue si < 85%
        assert accuracy >= 0.85, f'Performance insuffisante ({accuracy:.1%} < 85%)'
        "

    - name: Upload metrics
      uses: actions/upload-artifact@v3
      with:
        name: model-metrics
        path: metrics.json